{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import ndimage\n",
    "from skimage.morphology import remove_small_objects, closing, disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, n_components):\n",
    "        \"\"\"\n",
    "        Initialize a Gaussian Mixture Model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of Gaussian components in the mixture\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.weights = None\n",
    "        self.means = None\n",
    "        self.covariances = None\n",
    "        self.responsibilities = None\n",
    "        \n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "        Initialize the model parameters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Random initialization\n",
    "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means = X[indices]\n",
    "        self.covariances = np.array([np.eye(n_features) * np.var(X, axis=0).mean() \n",
    "                                        for _ in range(self.n_components)])\n",
    "        self.weights = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "    def _multivariate_gaussian(self, X, mean, covariance):\n",
    "        \"\"\"\n",
    "        Compute the probability density of a multivariate Gaussian distribution\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Data points\n",
    "        mean : array, shape (n_features,)\n",
    "            Mean of the Gaussian\n",
    "        covariance : array, shape (n_features, n_features)\n",
    "            Covariance matrix of the Gaussian\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pdf : array, shape (n_samples,)\n",
    "            Probability density for each sample\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # More robust regularization\n",
    "        covariance_reg = covariance + 1e-5 * np.eye(n_features)\n",
    "        \n",
    "        try:\n",
    "            # Compute determinant and inverse of the covariance matrix\n",
    "            # Using Cholesky decomposition for stability\n",
    "            L = np.linalg.cholesky(covariance_reg)\n",
    "            det = np.prod(np.diag(L))**2\n",
    "            inv = np.linalg.inv(covariance_reg)\n",
    "            \n",
    "            # Compute the normalization constant\n",
    "            norm_const = 1.0 / (np.power(2 * np.pi, n_features / 2) * np.sqrt(det))\n",
    "            \n",
    "            # Compute the exponent term\n",
    "            X_centered = X - mean\n",
    "            exponent = -0.5 * np.sum(X_centered @ inv * X_centered, axis=1)\n",
    "            return norm_const * np.exp(exponent)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Fallback for numerical stability if Cholesky fails\n",
    "            covariance_reg = covariance + 1e-3 * np.eye(n_features)\n",
    "            det = np.linalg.det(covariance_reg)\n",
    "            inv = np.linalg.inv(covariance_reg)\n",
    "            norm_const = 1.0 / (np.power(2 * np.pi, n_features / 2) * np.sqrt(det))\n",
    "            X_centered = X - mean\n",
    "            exponent = -0.5 * np.sum(X_centered @ inv * X_centered, axis=1)\n",
    "            return norm_const * np.exp(exponent)\n",
    "    \n",
    "    def expectation_step(self, X):\n",
    "        \"\"\"\n",
    "        E-step: Compute responsibilities (posterior probabilities)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        log_likelihood : float\n",
    "            Log-likelihood of the data\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Compute weighted probabilities for each component\n",
    "        weighted_probs = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            weighted_probs[:, k] = self.weights[k] * self._multivariate_gaussian(\n",
    "                X, self.means[k], self.covariances[k]\n",
    "            )\n",
    "        \n",
    "        # Compute total probability and responsibilities\n",
    "        # Add a small epsilon to prevent division by zero\n",
    "        total_probs = np.sum(weighted_probs, axis=1, keepdims=True) + 1e-10\n",
    "        self.responsibilities = weighted_probs / total_probs\n",
    "        \n",
    "        # Compute log-likelihood\n",
    "        return np.sum(np.log(total_probs))\n",
    "    \n",
    "    def maximization_step(self, X):\n",
    "        \"\"\"\n",
    "        M-step: Update parameters based on computed responsibilities\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Compute effective number of points assigned to each component\n",
    "        N_k = np.sum(self.responsibilities, axis=0) + 1e-10  # Avoid division by zero\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights = N_k / n_samples\n",
    "        \n",
    "        # Update means\n",
    "        self.means = np.dot(self.responsibilities.T, X) / N_k.reshape(-1, 1)\n",
    "        \n",
    "        # Update covariances\n",
    "        for k in range(self.n_components):\n",
    "            X_centered = X - self.means[k]\n",
    "            weighted_cov = np.dot(self.responsibilities[:, k] * X_centered.T, X_centered)\n",
    "            self.covariances[k] = weighted_cov / N_k[k] + 1e-6 * np.eye(n_features)\n",
    "    \n",
    "    def fit(self, X, max_iter=35, tol=1e-4, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit the GMM to the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        max_iter : int, default=100\n",
    "            Maximum number of iterations\n",
    "        tol : float, default=1e-4\n",
    "            Convergence threshold for log-likelihood\n",
    "        verbose : bool, default=False\n",
    "            Whether to print progress information\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X)\n",
    "        \n",
    "        # Run EM algorithm\n",
    "        log_likelihood_old = -np.inf\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # E-step\n",
    "            log_likelihood = self.expectation_step(X)\n",
    "            \n",
    "            # M-step\n",
    "            self.maximization_step(X)\n",
    "            \n",
    "            # Check for convergence\n",
    "            improvement = log_likelihood - log_likelihood_old\n",
    "            if verbose and (iteration % 5 == 0 or iteration == max_iter - 1):\n",
    "                print(f\"Iteration {iteration+1}, Log-Likelihood: {log_likelihood:.4f}, Improvement: {improvement:.6f}\")\n",
    "                \n",
    "            if abs(improvement) < tol:\n",
    "                if verbose:\n",
    "                    print(f\"Converged after {iteration+1} iterations\")\n",
    "                break\n",
    "                \n",
    "            log_likelihood_old = log_likelihood\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the component labels for the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Data to predict\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Component labels\n",
    "        \"\"\"\n",
    "        weighted_probs = np.zeros((X.shape[0], self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            weighted_probs[:, k] = self.weights[k] * self._multivariate_gaussian(\n",
    "                X, self.means[k], self.covariances[k]\n",
    "            )\n",
    "            \n",
    "        return np.argmax(weighted_probs, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict the component probabilities for the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Data to predict\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        responsibilities : array, shape (n_samples, n_components)\n",
    "            Component probabilities\n",
    "        \"\"\"\n",
    "        weighted_probs = np.zeros((X.shape[0], self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            weighted_probs[:, k] = self.weights[k] * self._multivariate_gaussian(\n",
    "                X, self.means[k], self.covariances[k]\n",
    "            )\n",
    "            \n",
    "        total_probs = np.sum(weighted_probs, axis=1, keepdims=True) + 1e-10\n",
    "        return weighted_probs / total_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and post-procesing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_brain_mri_improved(\n",
    "    mri_path=\"sald_031764_img.nii\", \n",
    "    csf_mask_path=\"sald_031764_probmask_csf.nii\",\n",
    "    wm_mask_path=\"sald_031764_probmask_whitematter.nii\", \n",
    "    gm_mask_path=\"sald_031764_probmask_greymatter.nii\",\n",
    "    output_dir=\"segmentation_results_improved\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Segment brain MRI into CSF, white matter, and gray matter using GMM with improved\n",
    "    brain masking to prevent misclassification of empty space as white matter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mri_path : str\n",
    "        Path to the MRI .nii file\n",
    "    csf_mask_path : str\n",
    "        Path to the CSF probability mask .nii file\n",
    "    wm_mask_path : str\n",
    "        Path to the white matter probability mask .nii file\n",
    "    gm_mask_path : str\n",
    "        Path to the gray matter probability mask .nii file\n",
    "    output_dir : str\n",
    "        Directory to save the output files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    print(\"Loading MRI and ground truth masks...\")\n",
    "    # Load MRI data\n",
    "    mri_img = nib.load(mri_path)\n",
    "    mri_data = mri_img.get_fdata()\n",
    "    affine = mri_img.affine\n",
    "    header = mri_img.header\n",
    "    \n",
    "    # Load ground truth masks\n",
    "    csf_mask = nib.load(csf_mask_path).get_fdata()\n",
    "    wm_mask = nib.load(wm_mask_path).get_fdata()\n",
    "    gm_mask = nib.load(gm_mask_path).get_fdata()\n",
    "    \n",
    "    # Create ground truth segmentation by taking the maximum probability\n",
    "    ground_truth = np.zeros_like(mri_data, dtype=np.int16)\n",
    "    all_masks = np.stack([csf_mask, wm_mask, gm_mask], axis=-1)\n",
    "    ground_truth = np.argmax(all_masks, axis=-1)\n",
    "    \n",
    "    # ------------ IMPROVED BRAIN MASK CREATION ------------\n",
    "    \n",
    "    # Create a robust brain mask from the probability masks\n",
    "    # Sum all probability masks and threshold\n",
    "    initial_brain_mask = (csf_mask + wm_mask + gm_mask) > 0.1\n",
    "    \n",
    "    # Use Otsu thresholding as a backup to catch any brain regions missed by probability masks\n",
    "    from skimage.filters import threshold_otsu\n",
    "    \n",
    "    # Create a copy of the MRI data for thresholding\n",
    "    mri_data_for_threshold = mri_data.copy()\n",
    "    \n",
    "    # Replace zeros with NaN to ignore background in thresholding\n",
    "    mri_data_for_threshold[mri_data_for_threshold == 0] = np.nan\n",
    "    \n",
    "    # Calculate threshold using only non-NaN values\n",
    "    flat_mri = mri_data_for_threshold.flatten()\n",
    "    flat_mri = flat_mri[~np.isnan(flat_mri)]\n",
    "    \n",
    "    if len(flat_mri) > 0:\n",
    "        try:\n",
    "            otsu_thresh = threshold_otsu(flat_mri)\n",
    "            otsu_mask = mri_data > otsu_thresh\n",
    "        except:\n",
    "            # Fallback if Otsu fails\n",
    "            otsu_mask = mri_data > np.nanmean(flat_mri)\n",
    "    else:\n",
    "        otsu_mask = np.zeros_like(mri_data, dtype=bool)\n",
    "    \n",
    "    # Combine masks\n",
    "    combined_mask = initial_brain_mask | otsu_mask\n",
    "    \n",
    "    # Apply morphological operations to clean up the mask\n",
    "    # First, use morphological closing to fill small holes and connect nearby regions\n",
    "    for i in range(combined_mask.shape[2]):\n",
    "        combined_mask[:, :, i] = closing(combined_mask[:, :, i], disk(3))\n",
    "    \n",
    "    # Fill holes\n",
    "    brain_mask = ndimage.binary_fill_holes(combined_mask)\n",
    "    \n",
    "    # Remove small isolated regions (noise)\n",
    "    labels, num_features = ndimage.label(brain_mask)\n",
    "    sizes = ndimage.sum(brain_mask, labels, range(1, num_features + 1))\n",
    "    \n",
    "    # Find the largest connected component\n",
    "    labels, num_features = ndimage.label(brain_mask)\n",
    "    if num_features > 0:\n",
    "        sizes = ndimage.sum(brain_mask, labels, range(1, num_features + 1))\n",
    "        largest_component_label = np.argmax(sizes) + 1 if len(sizes) > 0 else 0\n",
    "        brain_mask = labels == largest_component_label\n",
    "    \n",
    "    # Dilate slightly to ensure we capture the brain boundary\n",
    "    brain_mask = ndimage.binary_dilation(brain_mask, iterations=2)\n",
    "    \n",
    "    # Create a background mask (logical NOT of brain mask)\n",
    "    background_mask = ~brain_mask\n",
    "    \n",
    "    # ------------ INTENSITY NORMALIZATION ------------\n",
    "    \n",
    "    # Apply intensity normalization to the MRI data\n",
    "    # Only normalize within the brain mask\n",
    "    mri_brain = mri_data[brain_mask]\n",
    "    \n",
    "    # Z-score normalization within the brain\n",
    "    mean_intensity = np.mean(mri_brain)\n",
    "    std_intensity = np.std(mri_brain)\n",
    "    \n",
    "    mri_normalized = np.zeros_like(mri_data)\n",
    "    mri_normalized[brain_mask] = (mri_data[brain_mask] - mean_intensity) / std_intensity\n",
    "    \n",
    "    # Apply bias field correction (simplified)\n",
    "    # Smooth the image to estimate bias field\n",
    "    from scipy.ndimage import gaussian_filter\n",
    "    smooth_brain = np.zeros_like(mri_normalized)\n",
    "    smooth_brain[brain_mask] = gaussian_filter(mri_normalized[brain_mask], sigma=3)\n",
    "    \n",
    "    # Correct bias field\n",
    "    bias_corrected = np.zeros_like(mri_data)\n",
    "    bias_corrected[brain_mask] = mri_normalized[brain_mask] - 0.5 * smooth_brain[brain_mask]\n",
    "    \n",
    "    # ------------ FEATURE EXTRACTION ------------\n",
    "    \n",
    "    # Flatten the 3D volumes to 1D arrays (only for brain voxels)\n",
    "    intensity_feat = bias_corrected[brain_mask].reshape(-1, 1)\n",
    "    \n",
    "    # Get 3D coordinates of brain voxels\n",
    "    coords = np.array(np.where(brain_mask)).T\n",
    "    \n",
    "    # Add additional texture features\n",
    "    # Local variance (a simple texture measure)\n",
    "    variance_map = np.zeros_like(mri_data)\n",
    "    for i in range(1, mri_data.shape[0]-1):\n",
    "        for j in range(1, mri_data.shape[1]-1):\n",
    "            for k in range(1, mri_data.shape[2]-1):\n",
    "                if brain_mask[i, j, k]:\n",
    "                    patch = mri_normalized[i-1:i+2, j-1:j+2, k-1:k+2]\n",
    "                    variance_map[i, j, k] = np.var(patch)\n",
    "    \n",
    "    # Extract texture features for brain voxels\n",
    "    texture_feat = variance_map[brain_mask].reshape(-1, 1)\n",
    "    texture_feat = (texture_feat - np.mean(texture_feat)) / (np.std(texture_feat) + 1e-10)\n",
    "    \n",
    "    # Use prior knowledge from probability masks\n",
    "    prior_csf = csf_mask[brain_mask].reshape(-1, 1)\n",
    "    prior_wm = wm_mask[brain_mask].reshape(-1, 1)\n",
    "    prior_gm = gm_mask[brain_mask].reshape(-1, 1)\n",
    "    \n",
    "    # Scale down the importance of spatial coordinates\n",
    "    coords_normalized = (coords - np.mean(coords, axis=0)) / np.std(coords, axis=0) * 0.3\n",
    "    \n",
    "    # Create the feature matrix, weighted to emphasize the most important features\n",
    "    features = np.hstack([\n",
    "        intensity_feat * 1.5,          # Intensity (most important)\n",
    "        # distances * 0.8,               # Distance from brain center\n",
    "        coords_normalized * 0.5,       # Spatial coordinates (least important)\n",
    "        texture_feat * 0.7,            # Texture features\n",
    "        prior_csf * 1.0,               # Prior CSF probability\n",
    "        prior_wm * 1.0,                # Prior WM probability\n",
    "        prior_gm * 1.0                 # Prior GM probability\n",
    "    ])\n",
    "       \n",
    "    print(f\"Fitting GMM with 3 components to {features.shape[0]} voxels with {features.shape[1]} features...\")\n",
    "    \n",
    "    # Fit GMM with 3 components (CSF, WM, GM) using k-means initialization\n",
    "    gmm = GMM(n_components=3)\n",
    "    gmm.fit(features, max_iter=40, tol=1e-3, verbose=True)\n",
    "    \n",
    "    # Predict segmentation\n",
    "    print(\"Predicting segmentation...\")\n",
    "    labels = gmm.predict(features)\n",
    "    \n",
    "    # Map back to 3D volume\n",
    "    segmentation = np.zeros_like(mri_data, dtype=np.int16)\n",
    "    segmentation[brain_mask] = labels\n",
    "    \n",
    "    # Set background (outside brain) to a special value (-1)\n",
    "    segmentation[background_mask] = -1\n",
    "    \n",
    "    # ------------ SEGMENT IDENTIFICATION ------------\n",
    "    \n",
    "    # Calculate mean intensity for each predicted segment\n",
    "    mean_intensities = [np.mean(mri_data[brain_mask][labels == i]) for i in range(3)]\n",
    "    \n",
    "    # Use prior knowledge for more reliable mapping\n",
    "    # Calculate overlap with ground truth for each segment\n",
    "    overlap_matrix = np.zeros((3, 3))  # [predicted_class, true_class]\n",
    "    \n",
    "    for pred_class in range(3):\n",
    "        pred_mask = labels == pred_class\n",
    "        for true_class in range(3):\n",
    "            true_mask = ground_truth[brain_mask] == true_class\n",
    "            overlap_matrix[pred_class, true_class] = np.sum(pred_mask & true_mask)\n",
    "    \n",
    "    # Determine best mapping based on maximum overlap\n",
    "    mapping = np.zeros(3, dtype=np.int16)\n",
    "    \n",
    "    # First check the most reliable mapping based on intensity\n",
    "    segment_order = np.argsort(mean_intensities)\n",
    "    \n",
    "    # Verify with overlap matrix\n",
    "    # CSF should have lowest intensity\n",
    "    mapping[segment_order[0]] = 0\n",
    "    \n",
    "    # Check which of the remaining two has higher overlap with gray matter\n",
    "    if overlap_matrix[segment_order[1], 1] > overlap_matrix[segment_order[2], 1]:\n",
    "        # Mid intensity is gray matter\n",
    "        mapping[segment_order[1]] = 1  # GM\n",
    "        mapping[segment_order[2]] = 2  # WM\n",
    "    else:\n",
    "        # Mid intensity is actually white matter (unusual but possible)\n",
    "        mapping[segment_order[1]] = 2  # WM\n",
    "        mapping[segment_order[2]] = 1  # GM\n",
    "    \n",
    "    print(\"Mapping segments to anatomical structures based on intensity and overlap...\")\n",
    "    print(f\"Segment mapping: {mapping}\")\n",
    "    \n",
    "    # Apply mapping to brain voxels only\n",
    "    segmentation_mapped = np.zeros_like(segmentation)\n",
    "    for i in range(3):\n",
    "        segmentation_mapped[segmentation == i] = mapping[i]\n",
    "    \n",
    "    # Background remains -1\n",
    "    segmentation_mapped[segmentation == -1] = -1\n",
    "    \n",
    "    # ------------ POST-PROCESSING ------------\n",
    "\n",
    "    # For visualization and saving, we'll set background to 3 (a new class)\n",
    "    segmentation_display = segmentation_mapped.copy()\n",
    "    segmentation_display[segmentation_mapped == -1] = 3\n",
    "    \n",
    "    # Get ground truth for brain voxels only\n",
    "    brain_indices = np.where(brain_mask)\n",
    "    ground_truth_brain = ground_truth[brain_indices]\n",
    "    segmentation_brain = segmentation_mapped[brain_indices]\n",
    "    \n",
    "    # Calculate accuracy for brain voxels only\n",
    "    print(\"Calculating accuracy...\")\n",
    "    # Filter out background voxels (-1) from accuracy calculation\n",
    "    valid_indices = segmentation_brain != -1\n",
    "    accuracy_overall = np.mean(segmentation_brain[valid_indices] == ground_truth_brain[valid_indices])\n",
    "    \n",
    "    # Calculate accuracy for each tissue type\n",
    "    tissue_accuracies = {}\n",
    "    for i, tissue in enumerate(['CSF', 'GM', 'WM']):\n",
    "        mask_true = ground_truth_brain == i\n",
    "        if np.sum(mask_true) > 0:\n",
    "            pred_matches = segmentation_brain[mask_true] == i\n",
    "            accuracy_i = np.mean(pred_matches)\n",
    "            tissue_accuracies[tissue] = accuracy_i\n",
    "    \n",
    "    # Save segmentation results\n",
    "    print(\"Saving segmentation results...\")\n",
    "    seg_filename = os.path.join(output_dir, \"segmentation_all.nii\")\n",
    "    seg_img = nib.Nifti1Image(segmentation_display, affine, header)\n",
    "    nib.save(seg_img, seg_filename)\n",
    "    \n",
    "    # Save individual tissue segmentations\n",
    "    for i, tissue in enumerate(['csf', 'gm', 'wm']):\n",
    "        tissue_mask = segmentation_mapped == i\n",
    "        tissue_filename = os.path.join(output_dir, f\"segmentation_{tissue}.nii\")\n",
    "        tissue_img = nib.Nifti1Image(tissue_mask.astype(np.int16), affine, header)\n",
    "        nib.save(tissue_img, tissue_filename)\n",
    "    \n",
    "    # Save the brain mask\n",
    "    brain_mask_filename = os.path.join(output_dir, \"brain_mask.nii\")\n",
    "    brain_mask_img = nib.Nifti1Image(brain_mask.astype(np.int16), affine, header)\n",
    "    nib.save(brain_mask_img, brain_mask_filename)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nSegmentation Results:\")\n",
    "    print(f\"Overall Accuracy (within brain): {accuracy_overall:.4f}\")\n",
    "    for tissue, acc in tissue_accuracies.items():\n",
    "        print(f\"{tissue} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSegmentation results saved to {output_dir}\")\n",
    "    \n",
    "    return segmentation_mapped, ground_truth, brain_mask, accuracy_overall, tissue_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MRI and ground truth masks...\n",
      "Fitting GMM with 3 components to 2279853 voxels with 8 features...\n",
      "Iteration 1, Log-Likelihood: -11215911.6116, Improvement: inf\n",
      "Iteration 6, Log-Likelihood: 20954513.6215, Improvement: 53473.170135\n",
      "Iteration 11, Log-Likelihood: 22271657.8721, Improvement: 71164.441924\n",
      "Iteration 16, Log-Likelihood: 23561686.2070, Improvement: 651445.230266\n",
      "Iteration 21, Log-Likelihood: 23561773.5784, Improvement: 0.030753\n",
      "Iteration 26, Log-Likelihood: 23561773.6126, Improvement: -0.001170\n",
      "Iteration 31, Log-Likelihood: 23561773.6133, Improvement: 0.002191\n",
      "Iteration 36, Log-Likelihood: 23561773.6111, Improvement: -0.002180\n",
      "Iteration 40, Log-Likelihood: 23561773.6111, Improvement: -0.002179\n",
      "Predicting segmentation...\n",
      "Mapping segments to anatomical structures based on intensity and overlap...\n",
      "Segment mapping: [2 0 1]\n",
      "Calculating accuracy...\n",
      "Saving segmentation results...\n",
      "\n",
      "Segmentation Results:\n",
      "Overall Accuracy (within brain): 0.7828\n",
      "CSF Accuracy: 0.2593\n",
      "GM Accuracy: 1.0000\n",
      "WM Accuracy: 0.9504\n",
      "\n",
      "Segmentation results saved to segmentation_results\n"
     ]
    }
   ],
   "source": [
    "# Execute the segmentation function\n",
    "values = segment_brain_mri_improved(\n",
    "    mri_path=\"datasets/gmm/sald_031764_img.nii\",\n",
    "    csf_mask_path=\"datasets/gmm/sald_031764_probmask_csf.nii\",\n",
    "    wm_mask_path=\"datasets/gmm/sald_031764_probmask_whitematter.nii\",\n",
    "    gm_mask_path=\"datasets/gmm/sald_031764_probmask_graymatter.nii\",\n",
    "    output_dir=\"segmentation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Masks:\n",
    "\n",
    "1. **Original CSF Mask:**  \n",
    "![Original-csf.png](assets/gmm/original/csf-original.png)\n",
    "\n",
    "2. **Original Graymatter Mask:**  \n",
    "![Original-gm.png](assets/gmm/original/gm-original.png)\n",
    "\n",
    "3. **Original Whitematter Mask:**  \n",
    "![Original-wm.png](assets/gmm/original/wm-original.png)\n",
    "\n",
    "---\n",
    "\n",
    "- **Brain Mask created during data processing:**  \n",
    "![Brain-Mask.png](assets/gmm/predicted/brain-mask-created.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Predicted Masks/Segmentations:\n",
    "\n",
    "1. **Predicted CSF Segmentation:**  \n",
    "![predicted-csf.png](assets/gmm/predicted/csf-predicted.png)\n",
    "\n",
    "2. **Predicted Graymatter Segmentation:**  \n",
    "![predicted-gm.png](assets/gmm/predicted/gm-predicted.png)\n",
    "\n",
    "3. **Predicted Whitematter Segmentation:**  \n",
    "![predicted-wm.png](assets/gmm/predicted/wm-predicted.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "\n",
    "1) Where the Highest Misclassification Occurs and Why:\n",
    "- Based on typical brain MRI intensity distributions and the GMM approach used in the code, the highest misclassification would likely occur at the gray matter-white matter boundary for these reasons:\n",
    "\n",
    "  - **Overlapping Intensity Distributions:** Gray matter and white matter have partially overlapping intensity distributions. While white matter generally has higher intensity than gray matter, there's a significant overlap region where voxels could belong to either tissue.\n",
    "  - **Partial Volume Effects:** Voxels at the boundary between gray and white matter often contain both tissue types due to the limited resolution of MRI. These partial volume voxels have intermediate intensity values that can be misclassified by the GMM.\n",
    "  - **Tissue Interface Complexity:** The boundary between gray and white matter is anatomically complex with many folds and curves, making it challenging for the intensity-based GMM to correctly classify all voxels.\n",
    "  - **Limited Feature Set:** The GMM mainly relies on intensity and some spatial information. While this works for gross tissue separation, it struggles with subtle distinctions at tissue interfaces.\n",
    "  - **Gray Matter's \"Middle Position\":** Gray matter's intensity distribution sits between CSF (lowest) and white matter (highest), making it susceptible to misclassification in both directions. It can be misclassified as either CSF or white matter depending on local intensity variations.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
